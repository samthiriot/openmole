
@import org.openmole.site.tools._
@import org.openmole.site._
@import org.openmole.site.stylesheet._


@def modelAndVariables = """
val param1 = Val[Double]
val param2 = Val[Double]
val param3 = Val[Int]
val param4 = Val[String]
val distance1 = Val[Double]
val distance2 = Val[Double]
val output1 = Val[Double]
val output2 = Val[Double]
val modelTask = EmptyTask() set (
    inputs += (param1,param2, param3, param4),
    outputs += (output1, output2)
    )
"""


Using Genetic Algorithms (GA), OpenMOLE find the inputs set matching one or several criteria: this is called @b{calibration}.
In practice, calibration is used when you want to target @b{one} specific scenario or dynamic.
Usually, one uses a fitness function that is commensurable to a distance from your target, so if your model is not
 able to match the objective dynamic, the calibration will find the parameterization leading to the closest possible dynamic.
For more details on the calibration using genetic algorithms, see the @a("GA detailed page", href:=DocumentationPages.geneticAlgorithm.file).

@h2{Single criterion Calibration}

@break
@Resource.rawFrag(Resource.img.method.GAsingleID)
@break
@b{Method scores :}
@br
The single criterion Calibration method is @i{designed} to solve an optimization problem, so unsurprisingly it performs well
regarding the optimization grade.
Since it's only focused towards discover the best performing individual (parameter set), this method doesn't provide
insights about the sensitivity  of the model regarding its inputs, as it does not keep full record of the past input samplings leading to the optimal solution.
@br
For the same reason, this method does not intend to cover the input space and the output space and thus does not perform
well regarding the input space and output space exploration grades.
It @i{does} indeed explore input space, but does not keep track of it.
Calibration can handle stochasticity, using a @a("specific method", href:=DocumentationPages.stochasticityManagement.file).
@br
This method is not sensitive to the dimensionality of the model output space, as it only need a fitness value, which
  is a single criterion (value).
 It is, on the contrary, sensitive the input space dimensionality, as the population of potential solutions to
 evolve can be tremendous, as well as its fitness gradient hardly tractable.


@break
@figure
 @img(src := Resource.img.method.calibrationMono.file, center(70))
 @br
 Single criterion calibration answers the question :  For a given target value of output o1, what is(are) the
 parameter set(s) (i, j , k) that produce the closest values of o1 to the target ?
@break

@h3{Typed signature}
The single criterion calibration of a Model can be seen as a general function, whose signature can be typed, and be noted likewise:
@br@br
@div(style:="text-align:center")
    Calibration@sub{single} (X → Y) → Y → X

@br
With X , the Input space, Y, the output space.
@br
In other words : this function takes a model ( whose signature is (X→Y)  since it transforms inputs into outputs) , an element @i{y} of Y (y is the criterion value to reach) and  find an element of X (this element noted @i{x} happens to be such that , M(@b{x}) is the "closest" to @i{y})





@h2{Multi-criteria Calibration}

@br
@Resource.rawFrag(Resource.img.method.GAmultiID)
@break

@b{Method scores :}
@br
Multi-criteria  Calibration method slightly differs from the single criterion version.
It suffers the same limitation regarding Input and Output space limitations.
However, since it may reveal a Pareto frontier and the underlying trade-off, it reveals a little bit
of the model sensitivity, showing that the model performance regarding a criterion is  impacted by its performances
regarding the others. This is not @i{genuine sensitivity} as in sensitivity analysis, but still, it reveals a variation
of your model outputs, which is not bad after all !
@br

@break

@figure
  @img(src := Resource.img.method.calibrationMulti.file, center(70))
  @figcaption
    Figure: Multi-criteria answers the question : For a given target pattern (o1,o2) what are the parameters sets (i,j) that produce
    closest output values to the target pattern ? Sometimes a Pareto Frontier may appear !


@break



@p
  Calibration boils down to minimizing a distance measure between the
  model output and some data. When there is only a single distance measure
  considered, it is single criterion calibration. When it there are more
  than one distance that matter, it is multi-criteria calibration. For
  example, one may study a prey-predator model and want to find parameter
  values for which the model reproduce some expected size of both the prey
  and predator populations.

@p
  The single criterion case is simpler, because we can always tell which
  distance is smaller between any two distances. Thus, we can always
  select the set of parameter values that is the best.

@p
  In the multi-criteria case, it may not always be possible to tell which
  simulation output has the smallest distance to the data. For example,
  consider a pair (d1, d2) that represents the differences between the
  model output and the data for the prey population size (d1) and the
  predator population size (d2). Two pairs such as (10, 50) and (50,
  10) each have one element smaller than the other and one bigger. There
  is no natural way to tell which pair represents the smaller distance
  between the model and the data. Thus, in the multi-criteria case, we
  keep all the parameter sets (e.g. {(i1, j1, k1), (i2, j2, k2), ...})
  which yield distances (e.g. {(d11, d21), (d12, d22), ...}) for which
  we cannot find another parameter set that yields smaller distances for
  all the distances considered. The set of all these parameter sets is
  called the Pareto-front.

@break


@h3{Typed signature}
The multi-criterion calibration of a Model signature can be typed  likewise:
@br@br
@div(style:="text-align:center")
    Calibration@sub{multi}   :   (X → Y) → Y → [X]

@br
With X , the Input space, Y, the output space.
@br
In other words : this function takes a model M ( whose signature is (X→Y) ) , an element @i{y} of Y (y is the list of criterion value to reach) and  find a list of elements of X (noted @i{x}) such that M(x) are Pareto dominant compared to every image of other elements of X by M , regarding criterion @i{y})






@h2{Calibration with OpenMOLE}

Single and multi-criteria calibration in OpenMOLE are both done with the NSGA2 algorithm.
It takes the following parameters:

@ul
  @li{@hl.code("mu"): the population size,}
  @li{@hl.code("genome"): a list of the model parameters and their respective intervals,}
  @li{@hl.code("objectives"): a list of the distance measures (which in the single criterion case will contain only one measure)}
  @li{@hl.code("algorithm"): the nsga2 algorithm defining the evolution,}
  @li{@hl.code("evaluation"): the OpenMOLE task that runs the simulation,}
  @li{@hl.code("parallelism"): the number of simulations that will be run in parallel,}
  @li{@hl.code("termination"): the total number of evaluations (execution of the task passed to the parameter "evaluation") to be executed}

@p
  In your OpenMOLE script, the NSGA2 algorithm scheme is defined like so:
@br @hl.openmole("""
val nsga2 =
  NSGA2Evolution(
    evaluation = modelTask,
    parallelism = 10,
    termination = 100,
    genome = Seq(
      param1 in (0.0, 99.0),
      param2 in (0.0, 99.0),
      param3 in (0, 5),
      param4 in List("apple", "banana", "strawberry")),
    objectives = Seq(distance1, distance2)
  )
""", name = "Calibration")
where @hl.code("param1"), @hl.code("param2"), @hl.code("param3") and @hl.code("param4") are input of the @hl.code("modelTask"), and @hl.code("distance1") and @hl.code("distance2") are its outputs.
@p
The output of the genetic algorithm must be captured with a specific hook that saves the current optimal population: @code{SavePopulationHook}.
The arguments for a @code{SavePopulationHook} are the following:

@ul
  @li{@hl.code("evolution"): the genetic algorithm,}
  @li{@hl.code("dir"): the directory in which population files will be stored,}
  @li{@hl.code("frequency"): (@i{Optional}, Long) a frequency at which the generations will be saved.}

If you want only to get the final result and do not care about dynamics of the algorithm, you can use a @code{SaveLastPopulationHook} which saves the last population only and takes as arguments the algorithm and the file in which it will be saved.

A minimal working workflow is given below:

@br @hl.openmole("""
val param1 = Val[Double]
val param2 = Val[Double]
val param3 = Val[Int]
val param4 = Val[String]

val distance1 = Val[Double]
val distance2 = Val[Double]

val modelTask = ScalaTask("val distance1 = math.abs(param1 - 10.0) + math.abs(param2 - 50.0); val distance2 = (param3 == 2).toInt + (param4 == \"apple\").toInt") set (
    inputs += (param1,param2, param3, param4),
    outputs += (distance1, distance2)
    )

val nsga2 =
  NSGA2Evolution(
    evaluation = modelTask,
    parallelism = 10,
    termination = 100,
    genome = Seq(
      param1 in (0.0, 99.0),
      param2 in (0.0, 99.0),
      param3 in (0, 5),
      param4 in List("apple", "banana", "strawberry")),
    objectives = Seq(distance1, distance2)
  )

  val savePopulation = SavePopulationHook(nsga2, workDirectory / "evolution/")

  (nsga2 hook savePopulation on LocalEnvironment(4))

""", name = "Workflow nsga2")

@br

More details and advanced notions can be found on the @a("GA detailed page", href:=DocumentationPages.geneticAlgorithm.file).
